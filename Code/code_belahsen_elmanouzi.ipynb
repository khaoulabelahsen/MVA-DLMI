{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_belahsen_elmanouzi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSwUAkTkUW26",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "<h1><big><center>MVA DLMI : Deep Learning for Medical Imaging 2019/2020</center></big></h1>\n",
        "\n",
        "<h3><big><center> Khaoula Belahsen, Nadir El Manouzi </center></big></h3>\n",
        "\n",
        "\n",
        "<h2><big><center> Ultrasound Nerve Segmentation</center></big></h2>\n",
        "<h2><big><center> Final project code </center></big></h2>\n",
        "\n",
        "</br>\n",
        "<p align=\"center\">\n",
        "<img height=300px src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/5144/logos/front_page.png\"/></p>\n",
        "<p align=\"center\"></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osEVA0TgV9iD",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egnhVlueGoLD",
        "colab_type": "code",
        "outputId": "90883c4d-b422-44f0-bef2-9745e70c8544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/My Drive/DLMI/data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/DLMI/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4Re8WcWCJn",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Setting the directory "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFQDeOIvHDOp",
        "colab_type": "code",
        "outputId": "4cbab123-96a7-4aa1-934c-e0a7ea089ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# standard-module imports\n",
        "import os\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "\n",
        "_dir = os.path.abspath('')\n",
        "os.chdir(_dir)\n",
        "print(_dir)\n",
        "print(os.listdir(_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DLMI/data\n",
            "['np_data', 'sample_submission.csv', 'train_masks.csv', 'Untitled Folder', '.ipynb_checkpoints', 'train', 'test_ids.npy', 'np_data (1)', 'test', 'res']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Sv9t3jG2ZX",
        "colab_type": "code",
        "outputId": "4ba4835d-7bfc-4ac5-f8de-39c468c90cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# standard-module imports\n",
        "import os\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "\n",
        "_dir = os.path.abspath('')\n",
        "os.chdir(_dir)\n",
        "print(_dir)\n",
        "\n",
        "# data\n",
        "data_path = _dir\n",
        "preprocess_path = os.path.join(_dir, 'np_data')\n",
        "\n",
        "if not os.path.exists(preprocess_path):\n",
        "    os.mkdir(preprocess_path)\n",
        "print(os.listdir(_dir))\n",
        "\n",
        "# train data\n",
        "img_train_path = os.path.join(preprocess_path, 'imgs_train.npy')\n",
        "img_train_mask_path = os.path.join(preprocess_path, 'imgs_mask_train.npy')\n",
        "augmented_img_train_path = os.path.join(preprocess_path, 'augmented_imgs_train.npy')\n",
        "augmented_img_train_mask_path = os.path.join(preprocess_path, 'augmented_imgs_mask_train.npy')\n",
        "img_train_patients = os.path.join(preprocess_path, 'imgs_patient.npy')\n",
        "\n",
        "# test data\n",
        "img_test_path = os.path.join(preprocess_path, 'imgs_test.npy')\n",
        "img_test_id_path = os.path.join(preprocess_path, 'imgs_id_test.npy')\n",
        "\n",
        "print(os.listdir(preprocess_path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DLMI/data\n",
            "['np_data', 'sample_submission.csv', 'train_masks.csv', 'Untitled Folder', '.ipynb_checkpoints', 'train', 'test_ids.npy', 'np_data (1)', 'test', 'res']\n",
            "['imgs_patient.npy', 'imgs_train.npy', 'imgs_mask_train.npy', 'imgs_test.npy', 'imgs_id_test.npy', 'augmented_imgs_train.npy', 'augmented_imgs_mask_train.npy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSdgFvfiWLNv",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Load and create the data for train and test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwRmabpeHxsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ====================================================================================================================\n",
        "# Data\n",
        "# ====================================================================================================================\n",
        "\n",
        "# standard-module imports\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "image_rows = 420\n",
        "image_cols = 580\n",
        "\n",
        "\n",
        "def load_test_data():\n",
        "    \"\"\"Load test data from a .npy file.\n",
        "\n",
        "    :return: np.array with test data.\n",
        "    \"\"\"\n",
        "    print('Loading test data from %s' % img_test_path)\n",
        "    imgs_test = np.load(img_test_path)\n",
        "    return imgs_test\n",
        "\n",
        "\n",
        "def load_test_ids():\n",
        "    \"\"\"Load test ids from a .npy file.\n",
        "\n",
        "    :return: np.array with test ids. Shape (samples, ).\n",
        "    \"\"\"\n",
        "    print('Loading test ids from %s' % img_test_id_path)\n",
        "    imgs_id = np.load(img_test_id_path)\n",
        "    return imgs_id\n",
        "\n",
        "\n",
        "def load_train_data():\n",
        "    \"\"\"Load train data from a .npy file.\n",
        "\n",
        "    :return: np.array with train data.\n",
        "    \"\"\"\n",
        "    print('Loading train data from %s and %s' % (img_train_path, img_train_mask_path))\n",
        "    imgs_train = np.load(img_train_path)\n",
        "    imgs_mask_train = np.load(img_train_mask_path)\n",
        "    return imgs_train, imgs_mask_train\n",
        "\n",
        "def load_augmented_train_data():\n",
        "    \"\"\"Load train data from a .npy file.\n",
        "\n",
        "    :return: np.array with train data.\n",
        "    \"\"\"\n",
        "    print('Loading train data from %s and %s' % (augmented_img_train_path, augmented_img_train_mask_path))\n",
        "    imgs_train = np.load(augmented_img_train_path)\n",
        "    imgs_mask_train = np.load(augmented_img_train_mask_path)\n",
        "    return imgs_train, imgs_mask_train\n",
        "\n",
        "\n",
        "def load_patient_num():\n",
        "    \"\"\"Load the array with patient numbers from a .npy file\n",
        "\n",
        "    :return: np.array with patient numbers\n",
        "    \"\"\"\n",
        "    print('Loading patient numbers from %s' % img_train_patients)\n",
        "    return np.load(img_train_patients)\n",
        "\n",
        "\n",
        "def get_patient_nums(string):\n",
        "    \"\"\"Create a tuple (patient, photo) from image-file name patient_photo.tif\n",
        "\n",
        "    :param string: image-file name in string format: patient_photo.tif\n",
        "    :return: a tuple (patient, photo)\n",
        "\n",
        "    >>> get_patient_nums('32_50.tif')\n",
        "    (32, 50)\n",
        "    \"\"\"\n",
        "    patient, photo = string.split('_')\n",
        "    photo = photo.split('.')[0]\n",
        "    return int(patient), int(photo)\n",
        "\n",
        "\n",
        "def create_train_data():\n",
        "    \"\"\"\n",
        "    Create an np.array with patient numbers and save it into a .npy file.\n",
        "    Create an np.array with train images and save it into a .npy file.\n",
        "    Create an np.array with train masks and save it into a .npy file.\n",
        "\n",
        "    The np.array with patient numbers will have shape (samples, ).\n",
        "        So for each train image saved, the patient number will be recorded exactly in the same order the images were saved.\n",
        "    The np.array with train images will have shape (samples, rows, cols, channels).\n",
        "    The np.array with train masks will have shape (samples, rows, cols, channels).\n",
        "        The masks are saved in the same order as the images.\n",
        "    \"\"\"\n",
        "    train_data_path = os.path.join(data_path, 'train')\n",
        "    images = os.listdir(train_data_path)\n",
        "    total = len(images) // 2\n",
        "\n",
        "    imgs = np.ndarray((total, image_rows, image_cols, 1), dtype=np.uint8)\n",
        "    imgs_mask = np.ndarray((total, image_rows, image_cols, 1), dtype=np.uint8)\n",
        "    i = 0\n",
        "    print('Creating training images...')\n",
        "    img_patients = np.ndarray((total,), dtype=np.uint8)\n",
        "    for image_name in images:\n",
        "\n",
        "        # With \"continue\" skip the mask image in the iteration because the mask will be saved together with the image,\n",
        "        # when we get the image in one of the next iterations. This guarantees that the images, masks and corresponding\n",
        "        # patient numbers are all saved in the correct order.\n",
        "        if 'mask' in image_name:\n",
        "            continue\n",
        "\n",
        "        # we got to this point, meaning that image_name is a name of a training image and not a mask.\n",
        "\n",
        "        # recreate the mask's name fot this image\n",
        "        # noinspection PyTypeChecker\n",
        "        image_mask_name = image_name.split('.')[0] + '_mask.tif'\n",
        "        # get the patient number of the image\n",
        "        patient_num = image_name.split('_')[0]\n",
        "        # read the image itself to an np.array\n",
        "        img = cv2.imread(os.path.join(train_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
        "        # read the corresponding mask to an np.array\n",
        "        img_mask = cv2.imread(os.path.join(train_data_path, image_mask_name), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        imgs[i, :, :, 0] = img\n",
        "        imgs_mask[i, :, :, 0] = img_mask\n",
        "        img_patients[i] = patient_num\n",
        "        if i % 100 == 0:\n",
        "            print('Done: {0}/{1} images'.format(i, total))\n",
        "        i += 1\n",
        "    print('Loading done.')\n",
        "\n",
        "    # saving patient numbers\n",
        "    np.save(img_train_patients, img_patients)\n",
        "    # saving train images\n",
        "    np.save(img_train_path, imgs)\n",
        "    # saving train masks\n",
        "    np.save(img_train_mask_path, imgs_mask)\n",
        "    print('Saving to .npy files done.')\n",
        "\n",
        "\n",
        "def create_test_data():\n",
        "    \"\"\"\n",
        "    Create an np.array with test data and save it into a .npy file.\n",
        "    Create an np.array with ids for all images and save it into a .npy file.\n",
        "\n",
        "    The np.array with test data will have shape (samples, rows, cols, channels).\n",
        "    The np.array with test data ids will have shape (samples,). Each image id will be a number corresponding to the\n",
        "    number in a test image name. For example image '8.tif' will have 8 as its image id.\n",
        "    \"\"\"\n",
        "    test_data_path = os.path.join(data_path, 'test')\n",
        "    images = os.listdir(test_data_path)\n",
        "    total = len(images)\n",
        "\n",
        "    imgs = np.ndarray((total, image_rows, image_cols, 1), dtype=np.uint8)\n",
        "    imgs_id = np.ndarray((total,), dtype=np.int32)\n",
        "\n",
        "    i = 0\n",
        "    print('Creating test images...')\n",
        "    for image_name in images:\n",
        "        split = image_name.split('.')[0]\n",
        "        if split.endswith(\" (1)\"):\n",
        "          img_id = split[:-(len(\" (1)\"))]\n",
        "          img_id = int(img_id)\n",
        "        else : \n",
        "          img_id = int(image_name.split('.')[0])\n",
        "\n",
        "        img = cv2.imread(os.path.join(test_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        imgs[i, :, :, 0] = img\n",
        "        imgs_id[i] = img_id\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Done: {0}/{1} images'.format(i, total))\n",
        "        i += 1\n",
        "    print('Loading done.')\n",
        "\n",
        "    np.save(img_test_path, imgs)\n",
        "    np.save(img_test_id_path, imgs_id)\n",
        "    print('Saving to .npy files done.')\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     create_train_data()\n",
        "#     create_test_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Q4yKxX3A8d",
        "colab_type": "text"
      },
      "source": [
        "## 1.3  Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCtVj-R6Bazt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ====================================================================================================================\n",
        "# Data Augmentation\n",
        "# ====================================================================================================================\n",
        "import numpy as np\n",
        "import os, random, h5py\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "from skimage.transform import rotate\n",
        "from skimage.exposure import equalize_adapthist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def random_rotate(X,y,intensity):\n",
        "    Xf = np.array(X)\n",
        "    yf = np.array(y)\n",
        "    for i in range(X.shape[0]):\n",
        "        delta = 30. * intensity # scale using augmentation intensity\n",
        "        ran_delta = random.uniform(-delta, delta)\n",
        "        \n",
        "        Xf[i] = rotate(X[i], ran_delta, mode = 'constant', preserve_range=True)\n",
        "        yf[i] = rotate(y[i], ran_delta, mode = 'constant', preserve_range=True)\n",
        "    return Xf,yf\n",
        "\n",
        "def vertical_flip(X,y):\n",
        "    Xf = np.array(X)\n",
        "    yf = np.array(y)\n",
        "    for i in range(X.shape[0]):\n",
        "        Xf[i] = rotate(X[i], 180, mode='edge', preserve_range=True)\n",
        "        yf[i] = rotate(y[i], 180, mode='edge', preserve_range=True)\n",
        "    return Xf,yf\n",
        "    \n",
        "def random_flip(X,y):\n",
        "    ud_indices = random.sample(range(X.shape[0]), int(X.shape[0]/2))\n",
        "    lr_indices = random.sample(range(X.shape[0]), int(X.shape[0]/2))\n",
        "    Xf = np.array(X)\n",
        "    yf = np.array(y)\n",
        "    \n",
        "    Xf[ud_indices] = np.flipud(X[ud_indices])\n",
        "    yf[ud_indices] = np.flipud(y[ud_indices])\n",
        "    Xf[lr_indices] = np.fliplr(Xf[lr_indices])\n",
        "    yf[lr_indices] = np.fliplr(yf[lr_indices])\n",
        "    \n",
        "    return Xf,yf    \n",
        "\n",
        "def random_rotate90(X,y):\n",
        "    Xf = np.array(X)\n",
        "    yf = np.array(y)\n",
        "    for i in range(X.shape[0]):\n",
        "        rand_deg = np.random.randint(0,4) * 90 #0, 90, 180, 270 deg\n",
        "        \n",
        "        Xf[i] = rotate(X[i], rand_deg, mode='constant', preserve_range=True)\n",
        "        yf[i] = rotate(y[i], rand_deg, mode='constant', preserve_range=True)\n",
        "    return Xf,yf        \n",
        "\n",
        "def random_horizontal_flip(X,y):\n",
        "    flip_indices = random.sample(range(X.shape[0]), int(X.shape[0]/2))\n",
        "    Xf = np.array(X)\n",
        "    yf = np.array(y)\n",
        "    \n",
        "    Xf[flip_indices] = X[flip_indices, :, ::-1]\n",
        "    yf[flip_indices] = y[flip_indices, :, ::-1]\n",
        "    \n",
        "    return Xf,yf\n",
        "\n",
        "def vertical_flip_cv2(X,y):\n",
        "    Xf = np.array(X)\n",
        "    yf = np.array(y)\n",
        "    (h,w) = Xf.shape[1:]\n",
        "    center = (w/2, h/2)\n",
        "    for i in range(X.shape[0]):\n",
        "        M = cv2.getRotationMatrix2D(center, 180, 1.0)\n",
        "        \n",
        "        Xf[i] = cv2.warpAffine(X[i], M, (w,h), flags=cv2.INTER_LINEAR)\n",
        "        yf[i] = cv2.warpAffine(y[i], M, (w,h), flags=cv2.INTER_LINEAR)\n",
        "    return Xf,yf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANKQqh8T0J5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #test code\n",
        "# X_test, y_test = trainImages[0:5,:], trainMasks[0:5,:]\n",
        "# testX, testy = random_rotate(X_test, y_test,0.3)\n",
        "# testX, testy = random_flip(testX, testy)\n",
        "# testX, testy = random_rotate90(testX, testy)\n",
        "\n",
        "# plt.figure(300)\n",
        "# plt.subplot(221)\n",
        "# plt.imshow(X_test[3,:,:],cmap='gray')\n",
        "\n",
        "# plt.subplot(222)\n",
        "# plt.imshow(y_test[3,:,:],cmap='gray')\n",
        "\n",
        "# testX, testy = random_rotate(X_test, y_test,0.3)\n",
        "# plt.subplot(223)\n",
        "# plt.imshow(testX[3,:,:],cmap='gray')\n",
        "\n",
        "# plt.subplot(224)\n",
        "# plt.imshow(testy[3,:,:,1],cmap='gray')\n",
        "\n",
        "# testX, testy = random_flip(testX, testy)\n",
        "\n",
        "# plt.subplot(225)\n",
        "# plt.imshow(testX[3,:,:,1],cmap='gray')\n",
        "\n",
        "# plt.subplot(226)\n",
        "# plt.imshow(testy[3,:,:,1],cmap='gray')\n",
        "\n",
        "# testX, testy = vertical_flip(testX, testy)\n",
        "\n",
        "# plt.subplot(227)\n",
        "# plt.imshow(testX[3,:,:,1],cmap='gray')\n",
        "\n",
        "# plt.subplot(228)\n",
        "# plt.imshow(testy[3,:,:,1],cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGyv2IFR0RKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #test code\n",
        "# test_idx = random.randint(0,4)\n",
        "# plt.figure(200)\n",
        "# plt.subplot(221)\n",
        "# plt.imshow(X_test[test_idx,:,:],cmap='gray')\n",
        "\n",
        "# plt.subplot(222)\n",
        "# plt.imshow(y_test[test_idx,:,:],cmap='gray')\n",
        "\n",
        "# plt.subplot(223)\n",
        "# plt.imshow(testX[test_idx,:,:],cmap='gray')\n",
        "\n",
        "# plt.subplot(224)\n",
        "# plt.imshow(testy[test_idx,:,:],cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd_-tOzwDGVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_augmented_train_data():\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    imgs_train, imgs_mask_train = load_train_data()\n",
        "    # augmented_img_train, augmented_img_train_mask = random_rotate90(imgs_train, imgs_mask_train)\n",
        "    augmented_img_train, augmented_img_train_mask = random_flip(imgs_train, imgs_mask_train)\n",
        "    augmented_img_train, augmented_img_train_mask = vertical_flip(augmented_img_train, augmented_img_train_mask)\n",
        "    \n",
        "    augmented_img_train = np.concatenate((imgs_train, augmented_img_train), axis=0)\n",
        "    augmented_img_train_mask = np.concatenate((imgs_mask_train, augmented_img_train_mask), axis=0)\n",
        "\n",
        "    # saving train images\n",
        "    np.save(augmented_img_train_path, augmented_img_train)\n",
        "    # saving train masks\n",
        "    np.save(augmented_img_train_mask_path, augmented_img_train_mask)\n",
        "    print('Saving to .npy files done.')\n",
        "\n",
        "    print('Loading done.')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqwEhwnIWcmS",
        "colab_type": "text"
      },
      "source": [
        "# 2. Evaluation Metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDunHsTMKYpP",
        "colab_type": "code",
        "outputId": "dcb6e689-c1a9-4133-ab50-9d18521c2d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ====================================================================================================================\n",
        "# metrics\n",
        "# ====================================================================================================================\n",
        "\n",
        "# standard-module imports\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K  # tensorflow backend\n",
        "\n",
        "smooth = 1.\n",
        "\n",
        "\n",
        "def dice_coef(mask_1, mask_2, smooth=1):\n",
        "    \"\"\"Compute the dice coefficient between two equal-sized masks.\n",
        "\n",
        "    Dice Coefficient: https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
        "    We need to add smooth, because otherwise 2 empty (all zeros) masks will throw an error instead of giving 1 as an output.\n",
        "\n",
        "    :param mask_1: first mask\n",
        "    :param mask_2: second mask\n",
        "    :param smooth: Smoothing parameter for dice coefficient\n",
        "    :return: Smoothened dice coefficient between two equal-sized masks\n",
        "    \"\"\"\n",
        "    mask_1_flat = K.flatten(mask_1)\n",
        "    mask_2_flat = K.flatten(mask_2)\n",
        "\n",
        "    # for pixel values in {0, 1} multiplication is the intersection of masks\n",
        "    intersection = K.sum(mask_1_flat * mask_2_flat)\n",
        "    return (2. * intersection + smooth) / (K.sum(mask_1_flat) + K.sum(mask_2_flat) + smooth)\n",
        "\n",
        "\n",
        "\n",
        "def dice_coef_loss(mask_pred, mask_true):\n",
        "    \"\"\"Calculate dice coefficient loss, when comparing predicted mask for an image with the true mask\n",
        "\n",
        "    :param mask_pred: predicted mask\n",
        "    :param mask_true: true mask\n",
        "    :return: dice coefficient loss\n",
        "    \"\"\"\n",
        "    return -dice_coef(mask_pred, mask_true)\n",
        "\n",
        "\n",
        "def np_dice_coef(mask_1, mask_2, smooth=1):\n",
        "    \"\"\"Compute the dice coefficient between two equal-sized masks.\n",
        "    \n",
        "    Used for testing on artificially generated np.arrays\n",
        "\n",
        "    Dice Coefficient: https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
        "    We need to add smooth, because otherwise 2 empty (all zeros) masks will throw an error instead of giving 1 as an output.\n",
        "\n",
        "    :param mask_1: first mask\n",
        "    :param mask_2: second mask\n",
        "    :param smooth: Smoothing parameter for dice coefficient\n",
        "    :return: Smoothened dice coefficient between two equal-sized masks\n",
        "    \"\"\"\n",
        "    tr = mask_1.flatten()\n",
        "    pr = mask_2.flatten()\n",
        "    return (2. * np.sum(tr * pr) + smooth) / (np.sum(tr) + np.sum(pr) + smooth)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    a = np.random.random((420, 100))\n",
        "    b = np.random.random((420, 100))\n",
        "    res = np_dice_coef(a, b)\n",
        "    print(res)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5012933380156163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSQUnFW9Wf6G",
        "colab_type": "text"
      },
      "source": [
        "# 3. Models \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af0Xydu3W5Ss",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Unet, ResUnet and Unet two heads "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pYsdqqtTxv1",
        "colab_type": "code",
        "outputId": "bca0ecac-a84b-41ba-fa93-a9be751e28b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# ====================================================================================================================\n",
        "# u_model - needed for train\n",
        "# ====================================================================================================================\n",
        "\n",
        "# standard-module imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, add, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Dense, Conv2DTranspose, Activation \n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Lambda\n",
        "from tensorflow.keras.layers import ELU, LeakyReLU \n",
        "from tensorflow.keras.layers import concatenate \n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# # separate-module imports\n",
        "# from metric import dice_coef, dice_coef_loss\n",
        "\n",
        "\n",
        "K.set_image_data_format('channels_last')  # (number of images, rows per image, cols per image, channels)\n",
        "\n",
        "\n",
        "def inception_block(inputs, filters, split=False, activation='relu'):\n",
        "    \"\"\"Create an inception block with 2 options described in:\n",
        "    https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n",
        "\n",
        "    Default option: split=FALSE\n",
        "        Create an inception block described in v1, section b\n",
        "\n",
        "    Alternative option: split=TRUE\n",
        "        Create an inception block described in v2\n",
        "\n",
        "    :param inputs: Input 4D tensor (samples, rows, cols, channels)\n",
        "    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "    :param split: option of inception block\n",
        "    :param activation: activation function to use everywhere in the block\n",
        "    :return: output of the inception block, given inputs\n",
        "    \"\"\"\n",
        "    assert filters % 16 == 0\n",
        "    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n",
        "\n",
        "    #\n",
        "    # vertical 1\n",
        "    #\n",
        "    c1_1 = Conv2D(filters=filters // 4, kernel_size=(1, 1), kernel_initializer='he_normal', padding='same')(inputs)\n",
        "\n",
        " \n",
        "    #\n",
        "    # vertical 2\n",
        "    #\n",
        "    c2_1 = Conv2D(filters=filters // 8 * 3, kernel_size=(1, 1), kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    # no batch norm\n",
        "    c2_1 = actv()(c2_1)\n",
        "    if split:\n",
        "        c2_2 = Conv2D(filters=filters // 2, kernel_size=(1, 3), kernel_initializer='he_normal', padding='same')(c2_1)\n",
        "        c2_2 = BatchNormalization(axis=3)(c2_2)\n",
        "        c2_2 = actv()(c2_2)\n",
        "        c2_3 = Conv2D(filters=filters // 2, kernel_size=(3, 1), kernel_initializer='he_normal', padding='same')(c2_2)\n",
        "    else:\n",
        "        c2_3 = Conv2D(filters=filters // 2, kernel_size=(3, 3), kernel_initializer='he_normal', padding='same')(c2_1)\n",
        "\n",
        "    #\n",
        "    # vertical 3\n",
        "    #\n",
        "    c3_1 = Conv2D(filters=filters // 16, kernel_size=(1, 1), kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    # no batch norm\n",
        "    c3_1 = actv()(c3_1)\n",
        "    if split:\n",
        "        c3_2 = Conv2D(filters=filters // 8, kernel_size=(1, 5), kernel_initializer='he_normal', padding='same')(c3_1)\n",
        "        c3_2 = BatchNormalization(axis=3)(c3_2)  # mode=batch_mode # 0 in this case\n",
        "        c3_2 = actv()(c3_2)\n",
        "        c3_3 = Conv2D(filters=filters // 8, kernel_size=(5, 1), kernel_initializer='he_normal', padding='same')(c3_2)\n",
        "    else:\n",
        "        c3_3 = Conv2D(filters=filters // 8, kernel_size=(5, 5), kernel_initializer='he_normal', padding='same')(c3_1)\n",
        "\n",
        "    #\n",
        "    # vertical 4\n",
        "    #\n",
        "    p4_1 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    c4_2 = Conv2D(filters=filters // 8, kernel_size=(1, 1), kernel_initializer='he_normal', padding='same')(p4_1)\n",
        "\n",
        "    #\n",
        "    # concatenating verticals together\n",
        "    #\n",
        "    res = concatenate([c1_1, c2_3, c3_3, c4_2], axis=3)\n",
        "    res = BatchNormalization(axis=3)(res)\n",
        "    res = actv()(res)\n",
        "    return res\n",
        "\n",
        "\n",
        "# needed for rblock (residual block)\n",
        "def _shortcut(_input, residual):\n",
        "    stride_width = _input.shape[1] // residual.shape[1]\n",
        "    stride_height = _input.shape[2] // residual.shape[2]\n",
        "    equal_channels = residual.shape[3] == _input.shape[3]\n",
        "\n",
        "    shortcut = _input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual.shape[3], kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          kernel_initializer=\"he_normal\", padding=\"valid\")(_input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def rblock(inputs, kernel_size, filters, scale=0.1):\n",
        "    \"\"\"Create a scaled Residual block connecting the down-path and the up-path of the u-net architecture\n",
        "\n",
        "    Activations are scaled by a constant to prevent the network from dying. Usually is set between 0.1 and 0.3. See:\n",
        "    https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n",
        "\n",
        "    :param inputs: Input 4D tensor (samples, rows, cols, channels)\n",
        "    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution)\n",
        "    :param kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution\n",
        "                        window. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "    :param scale: scaling factor preventing the network from dying out\n",
        "    :return: output of a residual block\n",
        "    \"\"\"\n",
        "    residual = Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(inputs)\n",
        "    residual = BatchNormalization(axis=3)(residual)\n",
        "    residual = Lambda(lambda x: x * scale)(residual)\n",
        "    res = _shortcut(inputs, residual)\n",
        "    return ELU()(res)\n",
        "\n",
        "\n",
        "def NConv2D(filters, kernel_size, padding='same', strides=(1, 1)):\n",
        "    \"\"\"Create a (Normalized Conv2D followed by ELU activation) function\n",
        "    Conv2D -> BatchNormalization -> ELU()\n",
        "\n",
        "    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n",
        "    convolution)\n",
        "    :param kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution\n",
        "                        window. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "    :param padding: one of \"valid\" or \"same\" (case-insensitive)\n",
        "    :param strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height\n",
        "                    and width. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "                    Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n",
        "    :return: 2D Convolution function, followed by BatchNormalization across filters and ELU activation\n",
        "    \"\"\"\n",
        "\n",
        "    def f(_input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
        "                      padding=padding)(_input)\n",
        "        norm = BatchNormalization(axis=3)(conv)\n",
        "        return ELU()(norm)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "\n",
        "# Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\n",
        "def rec_res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n",
        "                  padding='same'):\n",
        "\n",
        "    input_n_filters = input_layer.get_shape().as_list()[3]\n",
        "\n",
        "    if out_n_filters != input_n_filters:\n",
        "        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding)(\n",
        "            input_layer)\n",
        "    else:\n",
        "        skip_layer = input_layer\n",
        "\n",
        "    layer = skip_layer\n",
        "    for j in range(2):\n",
        "\n",
        "        for i in range(2):\n",
        "            if i == 0:\n",
        "\n",
        "                layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding)(\n",
        "                    layer)\n",
        "                if batch_normalization:\n",
        "                    layer1 = BatchNormalization()(layer1)\n",
        "                layer1 = Activation('relu')(layer1)\n",
        "            layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding)(\n",
        "                add([layer1, layer]))\n",
        "            if batch_normalization:\n",
        "                layer1 = BatchNormalization()(layer1)\n",
        "            layer1 = Activation('relu')(layer1)\n",
        "        layer = layer1\n",
        "\n",
        "    out_layer = add([layer, skip_layer])\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "def up_and_concate(down_layer, layer):\n",
        "\n",
        "    in_channel = down_layer.get_shape().as_list()[3]\n",
        "\n",
        "    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n",
        "    up = UpSampling2D(size=(2, 2))(down_layer)\n",
        "\n",
        "    my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
        "\n",
        "    concate = my_concat([up, layer])\n",
        "\n",
        "    return concate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################\n",
        "###############################################\n",
        "############### MODELS ####################\n",
        "###############################################\n",
        "###############################################\n",
        "\n",
        "# U-net\n",
        "\n",
        "def get_unet_classic(optimizer):\n",
        "    \"\"\"\n",
        "    Creating and compiling the U-net\n",
        "\n",
        "    :param optimizer: specifies the optimiser for the u-net, e.g. Adam, RMSProp, etc.\n",
        "    :return: compiled u-net, Keras.Model object\n",
        "\n",
        "    Function taken in : https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py\n",
        "    \"\"\"\n",
        "\n",
        "    # act = 'relu'\n",
        "\n",
        "    #\n",
        "    # down the U-net\n",
        "    #\n",
        "    IMG_ROWS, IMG_COLS = 96, 96 # 80, 112\n",
        "\n",
        "    inputs = Input((IMG_ROWS, IMG_COLS, 1), name='main_input')\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    #\n",
        "    # bottom level of the U-net\n",
        "    #\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5) \n",
        "\n",
        "    #\n",
        "    # up the U-net\n",
        "    #\n",
        "\n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
        "\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
        "\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
        "\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)    \n",
        "\n",
        "    # output\n",
        "    conv10 = Conv2D(1, (1, 1), activation = 'sigmoid', name='main_output')(conv9)\n",
        "\n",
        "    # creating a model\n",
        "    model = Model(inputs=[inputs], outputs=[conv10])\n",
        "\n",
        "    # compiling the model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss={'main_output': dice_coef_loss},\n",
        "                  metrics=[dice_coef, tf.keras.metrics.Recall(), tf.keras.metrics.MeanIoU(num_classes=2)])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#########################################################################################################\n",
        "\n",
        "# U-net inception with two outputs (One auxiliary head for predicting the probability of nerve presence)\n",
        "\n",
        "def get_unet_inception_2head(optimizer):\n",
        "    \"\"\"\n",
        "    Creating and compiling the U-net\n",
        "\n",
        "    :param optimizer: specifies the optimiser for the u-net, e.g. Adam, RMSProp, etc.\n",
        "    :return: compiled u-net, Keras.Model object\n",
        "    \"\"\"\n",
        "\n",
        "    split = True\n",
        "    act = 'elu'\n",
        "\n",
        "    #\n",
        "    # down the U-net\n",
        "    #\n",
        "    IMG_ROWS, IMG_COLS = 80, 112 \n",
        "\n",
        "\n",
        "    inputs = Input((IMG_ROWS, IMG_COLS, 1), name='main_input')\n",
        "\n",
        "    conv1 = inception_block(inputs, 32, split=split, activation=act)\n",
        "    pool1 = NConv2D(32, kernel_size=(3, 3), strides=(2, 2))(conv1)\n",
        "    pool1 = Dropout(0.5)(pool1)\n",
        "\n",
        "\n",
        "    conv2 = inception_block(pool1, 64, split=split, activation=act)\n",
        "    pool2 = NConv2D(64, kernel_size=(3, 3), strides=(2, 2))(conv2)\n",
        "    pool2 = Dropout(0.5)(pool2)\n",
        "\n",
        "    conv3 = inception_block(pool2, 128, split=split, activation=act)\n",
        "    pool3 = NConv2D(128, kernel_size=(3, 3), strides=(2, 2))(conv3)\n",
        "    pool3 = Dropout(0.5)(pool3)\n",
        "\n",
        "    conv4 = inception_block(pool3, 256, split=split, activation=act)\n",
        "    pool4 = NConv2D(256, kernel_size=(3, 3), strides=(2, 2))(conv4)\n",
        "    pool4 = Dropout(0.5)(pool4)\n",
        "\n",
        "    #\n",
        "    # bottom level of the U-net\n",
        "    #\n",
        "\n",
        "    conv5 = inception_block(pool4, 512, split=split, activation=act)\n",
        "    conv5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    #\n",
        "    # auxiliary head for predicting probability of nerve presence\n",
        "    #\n",
        "\n",
        "    pre = Conv2D(1, kernel_size=(1, 1), kernel_initializer='he_normal', activation='sigmoid')(conv5)\n",
        "    pre = Flatten()(pre)\n",
        "    aux_out = Dense(1, activation='sigmoid', name='aux_output')(pre)\n",
        "\n",
        "    #\n",
        "    # up the U-net\n",
        "    #\n",
        "\n",
        "    after_conv4 = rblock(conv4, 1, 256)\n",
        "\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), after_conv4], axis=3)\n",
        "    conv6 = inception_block(up6, 256, split=split, activation=act)\n",
        "    conv6 = Dropout(0.5)(conv6)\n",
        "\n",
        "    after_conv3 = rblock(conv3, 1, 128)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), after_conv3], axis=3)\n",
        "    conv7 = inception_block(up7, 128, split=split, activation=act)\n",
        "    conv7 = Dropout(0.5)(conv7)\n",
        "\n",
        "\n",
        "    after_conv2 = rblock(conv2, 1, 64)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), after_conv2], axis=3)\n",
        "    conv8 = inception_block(up8, 64, split=split, activation=act)\n",
        "    conv8 = Dropout(0.5)(conv8)\n",
        "\n",
        "    after_conv1 = rblock(conv1, 1, 32)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), after_conv1], axis=3)\n",
        "    conv9 = inception_block(up9, 32, split=split, activation=act)\n",
        "    conv9 = Dropout(0.5)(conv9)\n",
        "\n",
        "    # output\n",
        "    conv10 = Conv2D(1, kernel_size=(1, 1), kernel_initializer='he_normal', activation='sigmoid', name='main_output')(\n",
        "        conv9)\n",
        "\n",
        "    # creating a model\n",
        "    model = Model(inputs=inputs, outputs=[conv10, aux_out])\n",
        "\n",
        "    # compiling the model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss={'main_output': dice_coef_loss, 'aux_output': 'binary_crossentropy'},\n",
        "                  metrics={'main_output': [dice_coef, tf.keras.metrics.Recall(), tf.keras.metrics.MeanIoU(num_classes=2)], 'aux_output': 'acc'},\n",
        "                  loss_weights={'main_output': 1., 'aux_output': 0.5})\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "# Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\n",
        "# adapted from https://github.com/lixiaolei1982/Keras-Implementation-of-U-Net-R2U-Net-Attention-U-Net-Attention-R2U-Net.-/blob/master/network.py\n",
        "\n",
        "def r2_unet(optimizer):\n",
        "\n",
        "  IMG_ROWS, IMG_COLS = 96, 96 \n",
        "  \n",
        "  inputs = Input((IMG_ROWS, IMG_COLS, 1), name='main_input')\n",
        "  x = inputs\n",
        "  depth = 4\n",
        "  features = 16 # 64, 32 and 16 tested\n",
        "  skips = []\n",
        "  for i in range(depth):\n",
        "    x = rec_res_block(x, features, batch_normalization=False)\n",
        "    skips.append(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "\n",
        "    features = features * 2\n",
        "\n",
        "  x = rec_res_block(x, features, batch_normalization=False)\n",
        "\n",
        "  for i in reversed(range(depth)):\n",
        "    features = features // 2\n",
        "    x = up_and_concate(x, skips[i])\n",
        "    x = rec_res_block(x, features, batch_normalization=False)\n",
        "\n",
        "  conv6 = Conv2D(1, (1,1), padding='same')(x)\n",
        "  conv7 = Activation('sigmoid')(conv6)\n",
        "\n",
        "  # creating a model\n",
        "  model = Model(inputs=[inputs], outputs=[conv7])\n",
        "\n",
        "  # compiling the model\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss= [dice_coef_loss],\n",
        "                # loss = 'binary_crossentropy',\n",
        "                metrics=[dice_coef, tf.keras.metrics.Recall(), tf.keras.metrics.MeanIoU(num_classes=2)]) \n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# get_unet = get_unet_classic\n",
        "# IMG_ROWS, IMG_COLS = 96, 96 # 80, 112 for the modified U-net (get_unet_inception_2head)\n",
        "\n",
        "\n",
        "get_unet = get_unet_inception_2head\n",
        "IMG_ROWS, IMG_COLS = 80, 112 \n",
        "\n",
        "\n",
        "# get_unet = r2_unet\n",
        "# IMG_ROWS, IMG_COLS = 96, 96 \n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    img_rows = IMG_ROWS\n",
        "    img_cols = IMG_COLS\n",
        "\n",
        "    # to check that model works without training, any kind of optimiser can be used\n",
        "    model = get_unet(Adam(lr=1e-5))\n",
        "\n",
        "    x = np.random.random((1, img_rows, img_cols, 1))\n",
        "    res = model.predict(x, 1)\n",
        "    print(res)\n",
        "    print('params', model.count_params())\n",
        "    print('layer num', len(model.layers))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs: (None, 96, 96, 1)\n",
            "conv1 (None, 96, 96, 32)\n",
            "pool1 (None, 48, 48, 32)\n",
            "conv2 (None, 48, 48, 64)\n",
            "pool2 (None, 24, 24, 64)\n",
            "conv3 (None, 24, 24, 128)\n",
            "pool3 (None, 12, 12, 128)\n",
            "conv4 (None, 12, 12, 256)\n",
            "pool4 (None, 6, 6, 256)\n",
            "conv5 (None, 6, 6, 512)\n",
            "up6 (None, 12, 12, 512)\n",
            "conv6 (None, 12, 12, 256)\n",
            "up7 (None, 24, 24, 256)\n",
            "conv7 (None, 24, 24, 128)\n",
            "up8 (None, 48, 48, 128)\n",
            "conv8 (None, 48, 48, 64)\n",
            "up9 (None, 96, 96, 64)\n",
            "conv9 (None, 96, 96, 32)\n",
            "conv10 (None, 96, 96, 1)\n",
            "[[[[0.49845898]\n",
            "   [0.49348718]\n",
            "   [0.49880686]\n",
            "   ...\n",
            "   [0.49630877]\n",
            "   [0.5005759 ]\n",
            "   [0.49840719]]\n",
            "\n",
            "  [[0.49526018]\n",
            "   [0.50039107]\n",
            "   [0.50046295]\n",
            "   ...\n",
            "   [0.49822032]\n",
            "   [0.4951882 ]\n",
            "   [0.49609646]]\n",
            "\n",
            "  [[0.49448922]\n",
            "   [0.49660355]\n",
            "   [0.49814498]\n",
            "   ...\n",
            "   [0.5000291 ]\n",
            "   [0.50018   ]\n",
            "   [0.5010441 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.49638504]\n",
            "   [0.49848908]\n",
            "   [0.5004627 ]\n",
            "   ...\n",
            "   [0.4988512 ]\n",
            "   [0.4978574 ]\n",
            "   [0.4964284 ]]\n",
            "\n",
            "  [[0.49604812]\n",
            "   [0.5045324 ]\n",
            "   [0.4967606 ]\n",
            "   ...\n",
            "   [0.5003365 ]\n",
            "   [0.4992992 ]\n",
            "   [0.49741983]]\n",
            "\n",
            "  [[0.4981226 ]\n",
            "   [0.49662152]\n",
            "   [0.5000853 ]\n",
            "   ...\n",
            "   [0.49709746]\n",
            "   [0.50057095]\n",
            "   [0.5009156 ]]]]\n",
            "params 7759521\n",
            "layer num 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBZnKRoYWqQa",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Unet ++ or Nested Unet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZifoG5rGhgk",
        "colab_type": "code",
        "outputId": "8a43f4b5-d11b-4ca7-dffb-1508786903d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Unet++ or Nested Unet \n",
        "# Adapted from https://github.com/CarryHJR/Nested-UNet/blob/master/model.py\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Standard UNet++ [Zhou et.al, 2018]\n",
        "Total params: 9,041,601\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.layers.noise import GaussianDropout\n",
        "# from keras.models import Model\n",
        "\n",
        "# from keras.layers import Input\n",
        "\n",
        "smooth = 1.\n",
        "dropout_rate = 0.3\n",
        "\n",
        "def standard_unit(input_tensor, stage, nb_filter, kernel_size=3):\n",
        "\n",
        "    act = 'elu'\n",
        "\n",
        "    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(input_tensor)\n",
        "    x = Dropout(dropout_rate, name='dp'+stage+'_1')(x)\n",
        "    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(x)\n",
        "    x = Dropout(dropout_rate, name='dp'+stage+'_2')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def Nest_Net(color_type=1, num_class=1, deep_supervision=False, lr = 2e-4):\n",
        "\n",
        "    IMG_ROWS, IMG_COLS = 96, 96 \n",
        "\n",
        "    nb_filter = [32,64,128,256,512]\n",
        "    act = 'elu'\n",
        "    bn_axis = 3\n",
        "\n",
        "    # Handle Dimension Ordering for different backends\n",
        "\n",
        "    img_input = Input(shape=(IMG_ROWS, IMG_COLS, color_type), name='main_input')\n",
        "\n",
        "    print(\"inputs:\", img_input.shape)\n",
        "\n",
        "    conv1_1 = standard_unit(img_input, stage='11', nb_filter=nb_filter[0])\n",
        "\n",
        "    print(\"conv1_1\", conv1_1.shape)\n",
        "\n",
        "    pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
        "\n",
        "    print(\"pool1\", pool1.shape)\n",
        "\n",
        "    conv2_1 = standard_unit(pool1, stage='21', nb_filter=nb_filter[1])\n",
        "\n",
        "    print(\"conv2_1\", conv2_1.shape)\n",
        "\n",
        "    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
        "\n",
        "    print(\"pool2\", pool2.shape)\n",
        "\n",
        "    up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
        "\n",
        "    print(\"up1_2\", up1_2.shape)\n",
        "    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n",
        "\n",
        "    print(\"conv1_2\", conv1_2.shape)\n",
        "\n",
        "    conv1_2 = standard_unit(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv3_1 = standard_unit(pool2, stage='31', nb_filter=nb_filter[2])\n",
        "    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
        "\n",
        "    up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
        "    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n",
        "    conv2_2 = standard_unit(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
        "    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n",
        "    conv1_3 = standard_unit(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv4_1 = standard_unit(pool3, stage='41', nb_filter=nb_filter[3])\n",
        "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
        "\n",
        "    up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
        "    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n",
        "    conv3_2 = standard_unit(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
        "    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n",
        "    conv2_3 = standard_unit(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
        "    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n",
        "    conv1_4 = standard_unit(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
        "\n",
        "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
        "\n",
        "    up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
        "    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n",
        "    conv4_2 = standard_unit(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
        "\n",
        "    up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
        "    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n",
        "    conv3_3 = standard_unit(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
        "\n",
        "    up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
        "    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n",
        "    conv2_4 = standard_unit(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
        "\n",
        "    up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
        "    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n",
        "    conv1_5 = standard_unit(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
        "\n",
        "    nestnet_output_1 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n",
        "    nestnet_output_2 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n",
        "    nestnet_output_3 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n",
        "    nestnet_output_4 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
        "\n",
        "    if deep_supervision:\n",
        "        model = Model(inputs = img_input, outputs=[nestnet_output_1,\n",
        "                                               nestnet_output_2,\n",
        "                                               nestnet_output_3,\n",
        "                                               nestnet_output_4])\n",
        "        model.compile(optimizer=Adam(2e-4), \n",
        "                     loss= {'output_1': dice_coef_loss, 'output_2': dice_coef_loss, \n",
        "                         'output_3' :dice_coef_loss , \n",
        "                         'output_4' : dice_coef_loss},\n",
        "                     metrics=[dice_coef, tf.keras.metrics.Recall(), tf.keras.metrics.MeanIoU(num_classes=2)]) \n",
        "    else:\n",
        "        model = Model(inputs = img_input, outputs =[nestnet_output_4])\n",
        "\n",
        "        # model.compile(optimizer=Adam(lr), \n",
        "        #               loss= [dice_coef_loss],\n",
        "        #               metrics=[dice_coef, tf.keras.metrics.Recall(), tf.keras.metrics.MeanIoU(num_classes=2)]) \n",
        "        \n",
        "        model.compile(loss= [dice_coef_loss],\n",
        "                  # here we add a regulizer normalization function from Talos\n",
        "                  optimizer=Adam(lr),\n",
        "                  metrics=[dice_coef, tf.keras.metrics.Recall(), tf.keras.metrics.MeanIoU(num_classes=2)])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BwVT158WzCj",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Attention Residual Unet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOJQa5NwGXp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Raw implementation with AttResUnet : U-Net model with residual block, using the spatial-level attention gate.\n",
        "# Adapted from https://github.com/MoleImg/Attention_UNet/blob/master/AttResUNet.py\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Standard AttResUnet [Oktay et.al, 2018, Alom et.al]\n",
        "Total params: 9,786,857\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "FILTER_NUM = 32 # number of basic filters for the first layer\n",
        "FILTER_SIZE = 3 # size of the convolutional filter\n",
        "DOWN_SAMP_SIZE = 2 # size of pooling filters\n",
        "UP_SAMP_SIZE = 2 # size of upsampling filters\n",
        "OUTPUT_MASK_CHANNEL = 1\n",
        "IMG_ROWS, IMG_COLS = 96, 96 # 80, 112\n",
        "\n",
        "\n",
        "def expend_as(tensor, rep):\n",
        "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
        "                          arguments={'repnum': rep})(tensor)\n",
        "\n",
        "\n",
        "def double_conv_layer(x, filter_size, size, dropout, batch_norm=False):\n",
        "    '''\n",
        "    construction of a double convolutional layer using\n",
        "    SAME padding\n",
        "    RELU nonlinear activation function\n",
        "    :param x: input\n",
        "    :param filter_size: size of convolutional filter\n",
        "    :param size: number of filters\n",
        "    :param dropout: FLAG & RATE of dropout.\n",
        "            if < 0 dropout cancelled, if > 0 set as the rate\n",
        "    :param batch_norm: flag of if batch_norm used,\n",
        "            if True batch normalization\n",
        "    :return: output of a double convolutional layer\n",
        "    '''\n",
        "    axis = 3\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)\n",
        "    if batch_norm is True:\n",
        "        conv = layers.BatchNormalization(axis=axis)(conv)\n",
        "    conv = layers.Activation('relu')(conv)\n",
        "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n",
        "    if batch_norm is True:\n",
        "        conv = layers.BatchNormalization(axis=axis)(conv)\n",
        "    conv = layers.Activation('relu')(conv)\n",
        "    if dropout > 0:\n",
        "        conv = layers.Dropout(dropout)(conv)\n",
        "\n",
        "    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n",
        "    if batch_norm is True:\n",
        "        shortcut = layers.BatchNormalization(axis=axis)(shortcut)\n",
        "\n",
        "    res_path = layers.add([shortcut, conv])\n",
        "    return res_path\n",
        "\n",
        "def gating_signal(input, out_size, batch_norm=False):\n",
        "    \"\"\"\n",
        "    resize the down layer feature map into the same dimension as the up layer feature map\n",
        "    using 1x1 conv\n",
        "    :param input:   down-dim feature map\n",
        "    :param out_size:output channel number\n",
        "    :return: the gating feature map with the same dimension of the up layer feature map\n",
        "    \"\"\"\n",
        "    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def attention_block(x, gating, inter_shape):\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gating)\n",
        "\n",
        "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)\n",
        "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
        "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
        "                                 padding='same')(phi_g)  # 16\n",
        "\n",
        "    concat_xg = layers.add([upsample_g, theta_x])\n",
        "    act_xg = layers.Activation('relu')(concat_xg)\n",
        "    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n",
        "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = expend_as(upsample_psi, shape_x[3])\n",
        "\n",
        "    y = layers.multiply([upsample_psi, x])\n",
        "\n",
        "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
        "    result_bn = layers.BatchNormalization()(result)\n",
        "    return result_bn\n",
        "\n",
        "\n",
        "def Attention_ResUNet(optimizer, dropout_rate=0.5, batch_norm=True):\n",
        "    '''\n",
        "    Rsidual UNet construction, with attention gate\n",
        "    convolution: 3*3 SAME padding\n",
        "    pooling: 2*2 VALID padding\n",
        "    upsampling: 3*3 VALID padding\n",
        "    final convolution: 1*1\n",
        "    :param dropout_rate: FLAG & RATE of dropout.\n",
        "            if < 0 dropout cancelled, if > 0 set as the rate\n",
        "    :param batch_norm: flag of if batch_norm used,\n",
        "            if True batch normalization\n",
        "    :return: model\n",
        "    '''\n",
        "\n",
        "    color_type = 1 \n",
        "    # input data\n",
        "    # dimension of the image depth\n",
        "    inputs = layers.Input(shape=(IMG_ROWS, IMG_COLS, color_type), name='main_input')\n",
        "    axis = 3\n",
        "\n",
        "    # Downsampling layers\n",
        "    # DownRes 1, double residual convolution + pooling\n",
        "    conv_128 = double_conv_layer(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
        "    # DownRes 2\n",
        "    conv_64 = double_conv_layer(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
        "    # DownRes 3\n",
        "    conv_32 = double_conv_layer(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
        "    # DownRes 4\n",
        "    conv_16 = double_conv_layer(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
        "    # DownRes 5, convolution only\n",
        "    conv_8 = double_conv_layer(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # Upsampling layers\n",
        "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
        "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
        "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
        "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
        "    up_16 = layers.concatenate([up_16, att_16], axis=axis)\n",
        "    up_conv_16 = double_conv_layer(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 7\n",
        "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
        "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
        "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
        "    up_32 = layers.concatenate([up_32, att_32], axis=axis)\n",
        "    up_conv_32 = double_conv_layer(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 8\n",
        "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
        "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
        "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
        "    up_64 = layers.concatenate([up_64, att_64], axis=axis)\n",
        "    up_conv_64 = double_conv_layer(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 9\n",
        "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
        "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
        "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
        "    up_128 = layers.concatenate([up_128, att_128], axis=axis)\n",
        "    up_conv_128 = double_conv_layer(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    # valid padding\n",
        "    # batch normalization\n",
        "    # sigmoid nonlinear activation\n",
        "    conv_final = layers.Conv2D(OUTPUT_MASK_CHANNEL, kernel_size=(1,1))(up_conv_128)\n",
        "    conv_final = layers.BatchNormalization(axis=axis)(conv_final)\n",
        "    conv_final = layers.Activation('relu')(conv_final)\n",
        "\n",
        "    # Model integration\n",
        "    model = models.Model(inputs, conv_final, name=\"AttentionResUNet\")\n",
        "    # optim = Adam() # optimizer\n",
        "    loss = dice_coef_loss # loss function\n",
        "    metrics = [dice_coef]\n",
        "    model.compile(optimizer=optimizer, loss=dice_coef_loss, \n",
        "                  metrics=[dice_coef, tf.keras.metrics.Recall()\n",
        "                  , tf.keras.metrics.MeanIoU(num_classes=2)\n",
        "                  ]) # configuration\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QICWGEdF3Ksv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_unet(Adam()).summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYThP1a32BCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Nest_Net().summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2SwuqOaIA4m",
        "colab_type": "code",
        "outputId": "7f07e282-11d7-4551-a11e-171632bee453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Attention_ResUNet(Adam()).summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"AttentionResUNet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "main_input (InputLayer)         [(None, 96, 96, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 96, 96, 32)   320         main_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 96, 96, 32)   128         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 96, 96, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 96, 96, 32)   9248        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 96, 96, 32)   128         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 96, 96, 32)   64          main_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 96, 96, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 96, 96, 32)   128         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 96, 96, 32)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 96, 96, 32)   0           batch_normalization_2[0][0]      \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 48, 48, 32)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 48, 48, 64)   18496       max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 48, 48, 64)   256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 48, 48, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 48, 48, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 48, 48, 64)   256         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 48, 48, 64)   2112        max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 48, 48, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 48, 48, 64)   256         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 48, 48, 64)   0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 48, 48, 64)   0           batch_normalization_5[0][0]      \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 24, 24, 64)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 24, 24, 128)  73856       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 24, 24, 128)  512         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 24, 24, 128)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 24, 24, 128)  147584      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 24, 24, 128)  512         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 24, 24, 128)  8320        max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 24, 24, 128)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 24, 24, 128)  512         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 24, 24, 128)  0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 24, 24, 128)  0           batch_normalization_8[0][0]      \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 12, 12, 128)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 12, 12, 256)  295168      max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 12, 12, 256)  1024        conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 12, 12, 256)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 12, 12, 256)  590080      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 12, 12, 256)  1024        conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 12, 12, 256)  33024       max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 12, 12, 256)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 12, 12, 256)  1024        conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 12, 12, 256)  0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 12, 12, 256)  0           batch_normalization_11[0][0]     \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 6, 6, 256)    0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 6, 6, 512)    1180160     max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 6, 6, 512)    2048        conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 6, 6, 512)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 6, 6, 512)    2359808     activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 6, 6, 512)    2048        conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 6, 6, 512)    131584      max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 6, 6, 512)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 6, 6, 512)    2048        conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 6, 6, 512)    0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 6, 6, 512)    0           batch_normalization_14[0][0]     \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 6, 6, 256)    131328      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 6, 6, 256)    1024        conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 6, 6, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 6, 6, 256)    65792       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 6, 6, 256)    590080      conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 6, 6, 256)    262400      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 6, 6, 256)    0           conv2d_transpose_8[0][0]         \n",
            "                                                                 conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 6, 6, 256)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 6, 6, 1)      257         activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 6, 6, 1)      0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 12, 12, 1)    0           activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 12, 12, 256)  0           up_sampling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 12, 12, 256)  0           lambda[0][0]                     \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 12, 12, 256)  65792       multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 12, 12, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 12, 12, 256)  1024        conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 12, 12, 768)  0           up_sampling2d_1[0][0]            \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 12, 12, 256)  1769728     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 12, 12, 256)  1024        conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 12, 12, 256)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 12, 12, 256)  590080      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 12, 12, 256)  1024        conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 12, 12, 256)  196864      concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 12, 12, 256)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 12, 12, 256)  1024        conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 12, 12, 256)  0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 12, 12, 256)  0           batch_normalization_19[0][0]     \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 12, 12, 128)  32896       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 12, 12, 128)  512         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 12, 12, 128)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 12, 12, 128)  16512       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTrans (None, 12, 12, 128)  147584      conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 12, 12, 128)  65664       add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 12, 12, 128)  0           conv2d_transpose_9[0][0]         \n",
            "                                                                 conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 12, 12, 128)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 12, 12, 1)    129         activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 12, 12, 1)    0           conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 24, 24, 1)    0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 24, 24, 128)  0           up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 24, 24, 128)  0           lambda_1[0][0]                   \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 24, 24, 128)  16512       multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 24, 24, 256)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 24, 24, 128)  512         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 24, 24, 384)  0           up_sampling2d_3[0][0]            \n",
            "                                                                 batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 24, 24, 128)  442496      concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 24, 24, 128)  512         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 24, 24, 128)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 24, 24, 128)  147584      activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 24, 24, 128)  512         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 24, 24, 128)  49280       concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 24, 24, 128)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 24, 24, 128)  512         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 24, 24, 128)  0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 24, 24, 128)  0           batch_normalization_24[0][0]     \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 24, 24, 64)   8256        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 24, 24, 64)   256         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 24, 24, 64)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 24, 24, 64)   4160        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DTran (None, 24, 24, 64)   36928       conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 24, 24, 64)   16448       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 24, 24, 64)   0           conv2d_transpose_10[0][0]        \n",
            "                                                                 conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 24, 24, 64)   0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 24, 24, 1)    65          activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 24, 24, 1)    0           conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 48, 48, 1)    0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 48, 48, 64)   0           up_sampling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 48, 48, 64)   0           lambda_2[0][0]                   \n",
            "                                                                 add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 48, 48, 64)   4160        multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 48, 48, 128)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 48, 48, 64)   256         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 48, 48, 192)  0           up_sampling2d_5[0][0]            \n",
            "                                                                 batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 48, 48, 64)   110656      concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 48, 48, 64)   256         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 48, 48, 64)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 48, 48, 64)   36928       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 48, 48, 64)   256         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 48, 48, 64)   12352       concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 48, 48, 64)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 48, 48, 64)   256         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 48, 48, 64)   0           activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 48, 48, 64)   0           batch_normalization_29[0][0]     \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 48, 48, 32)   2080        add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 48, 48, 32)   128         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 48, 48, 32)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 48, 48, 32)   1056        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DTran (None, 48, 48, 32)   9248        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 48, 48, 32)   4128        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 48, 48, 32)   0           conv2d_transpose_11[0][0]        \n",
            "                                                                 conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 48, 48, 32)   0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 48, 48, 1)    33          activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 48, 48, 1)    0           conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 96, 96, 1)    0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 96, 96, 32)   0           up_sampling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 96, 96, 32)   0           lambda_3[0][0]                   \n",
            "                                                                 add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 96, 96, 32)   1056        multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 96, 96, 64)   0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 96, 96, 32)   128         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 96, 96, 96)   0           up_sampling2d_7[0][0]            \n",
            "                                                                 batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 96, 96, 32)   27680       concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 96, 96, 32)   128         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 96, 96, 32)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 96, 96, 32)   9248        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 96, 96, 32)   128         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 96, 96, 32)   3104        concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 96, 96, 32)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 96, 96, 32)   128         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 96, 96, 32)   0           activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 96, 96, 32)   0           batch_normalization_34[0][0]     \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 96, 96, 1)    33          add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 96, 96, 1)    4           conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 96, 96, 1)    0           batch_normalization_35[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 9,786,857\n",
            "Trainable params: 9,776,103\n",
            "Non-trainable params: 10,754\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brZHMRSMXYB7",
        "colab_type": "text"
      },
      "source": [
        "# 4. Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCL0RJhfUUFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ====================================================================================================================\n",
        "# utils  - needed for train\n",
        "# ====================================================================================================================\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_pickle(file_path):\n",
        "    data = None\n",
        "    with open(file_path, \"rb\") as dumpFile:\n",
        "        data = pickle.load(dumpFile)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_pickle(file_path, data):\n",
        "    with open(file_path, \"wb\") as dumpFile:\n",
        "        pickle.dump(data, dumpFile, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def count_enum(words):\n",
        "    wdict = {}\n",
        "    get = wdict.get\n",
        "    for word in words:\n",
        "        wdict[word] = get(word, 0) + 1\n",
        "    return wdict\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6l9ASAPMJ0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "7aedffca-baf1-49d5-dc08-6686b8dbcfb8"
      },
      "source": [
        "# ====================================================================================================================\n",
        "# train\n",
        "# ====================================================================================================================\n",
        "\n",
        "# standard-module imports\n",
        "import numpy as np\n",
        "import cv2, os, shutil, random\n",
        "from optparse import OptionParser\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "# # separate-module imports\n",
        "#\n",
        "# from u_model import get_unet, IMG_COLS as img_cols, IMG_ROWS as img_rows\n",
        "# from data import load_train_data, load_test_data, load_patient_num\n",
        "# from utils import save_pickle, load_pickle, count_enum\n",
        "\n",
        "\n",
        "def preprocess(imgs, to_rows=None, to_cols=None):\n",
        "    \"\"\"Resize all images in a 4D tensor of images of the shape (samples, rows, cols, channels).\n",
        "\n",
        "    :param imgs: a 4D tensor of images of the shape (samples, rows, cols, channels)\n",
        "    :param to_rows: new number of rows for images to be resized to\n",
        "    :param to_cols: new number of rows for images to be resized to\n",
        "    :return: a 4D tensor of images of the shape (samples, to_rows, to_cols, channels)\n",
        "    \"\"\"\n",
        "    if to_rows is None or to_cols is None:\n",
        "        to_rows = img_rows\n",
        "        to_cols = img_cols\n",
        "\n",
        "    print(imgs.shape)\n",
        "    imgs_p = np.ndarray((imgs.shape[0], to_rows, to_cols, imgs.shape[3]), dtype=np.uint8)\n",
        "    for i in range(imgs.shape[0]):\n",
        "        imgs_p[i, :, :, 0] = cv2.resize(imgs[i, :, :, 0], (to_cols, to_rows), interpolation=cv2.INTER_CUBIC)\n",
        "    return imgs_p\n",
        "\n",
        "\n",
        "def get_object_existence(mask_array):\n",
        "    \"\"\"Create an array specifying nerve presence on each of the masks in the mask_array\n",
        "\n",
        "    :param mask_array: 4D tensor of a shape (samples, rows, cols, channels=1) with masks\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print(\"type(mask_array):\", type(mask_array))\n",
        "    print(\"mask_array.shape:\", mask_array.shape)\n",
        "    return np.array([int(np.sum(mask_array[i, :, :, 0]) > 0) for i in range(mask_array.shape[0])])\n",
        "\n",
        "\n",
        "def load_pretrained_model(model, pretrained_path):\n",
        "    # load pretrained model from a given path, if there is a pretrained model\n",
        "    if pretrained_path is not None:\n",
        "        if not os.path.exists(pretrained_path):\n",
        "            raise ValueError('No such pre-trained path exists')\n",
        "        model.load_weights(pretrained_path)\n",
        "\n",
        "\n",
        "class Learner:\n",
        "    \"\"\"Perform training on the train data and predicting on the test data\n",
        "\n",
        "    :ivar model_func: function which creates the network architecture and compiles the Model, e.g. get_unet\n",
        "    :ivar validation_split: train/validation split used for training\n",
        "    :ivar mean: mean of data np.array passed to the _init_mean_std(self, data) function\n",
        "    :ivar std: std of data np.array passed to the _init_mean_std(self, data) function\n",
        "\n",
        "    :ivar __iter_res_dir: iterative results directory\n",
        "    :ivar __iter_res_file: iterative results file in which we'll write the results: epochs and validation losses\n",
        "    \"\"\"\n",
        "\n",
        "    # class variables\n",
        "    suffix = ''\n",
        "    res_dir = os.path.join(_dir, 'res' + suffix)\n",
        "    best_weight_path = os.path.join(res_dir, 'nestnet_aug.hdf5')\n",
        "    test_mask_res = os.path.join(res_dir, 'imgs_mask_test.npy')\n",
        "    test_mask_exist_res = os.path.join(res_dir, 'imgs_mask_exist_test.npy')\n",
        "    meanstd_path = os.path.join(res_dir, 'meanstd.dump')\n",
        "    valid_data_path = os.path.join(res_dir, 'valid.npy')\n",
        "    tensorboard_dir = os.path.join(res_dir, 'tb')\n",
        "\n",
        "    def __init__(self, model_func, validation_split):\n",
        "        self.model_func = model_func\n",
        "        self.validation_split = validation_split\n",
        "        self.__iter_res_dir = os.path.join(self.res_dir, 'res_iter')\n",
        "        self.__iter_res_file = os.path.join(self.__iter_res_dir, '{epoch:02d}-{val_loss:.4f}.unet.hdf5')\n",
        "\n",
        "    def _dir_init(self):\n",
        "        # create results directory if it does not exist, if it does - deletes it with everything inside and creates again\n",
        "\n",
        "        if not os.path.exists(self.res_dir):\n",
        "            os.mkdir(self.res_dir)\n",
        "        # iter clean\n",
        "        if os.path.exists(self.__iter_res_dir):\n",
        "            shutil.rmtree(self.__iter_res_dir)\n",
        "        os.mkdir(self.__iter_res_dir)\n",
        "\n",
        "    def save_meanstd(self):\n",
        "        \"\"\"Save mean and standard deviation\"\"\"\n",
        "        data = [self.mean, self.std]\n",
        "        save_pickle(self.meanstd_path, data)\n",
        "\n",
        "    @classmethod\n",
        "    def load_meanstd(cls):\n",
        "        \"\"\"Load mean and standard deviation\n",
        "\n",
        "        :return: tuple (mean, standard deviation)\n",
        "        \"\"\"\n",
        "        print('Load meanstd from %s' % cls.meanstd_path)\n",
        "        mean, std = load_pickle(cls.meanstd_path)\n",
        "        return mean, std\n",
        "\n",
        "    @classmethod\n",
        "    def save_valid_idx(cls, idx):\n",
        "        save_pickle(cls.valid_data_path, idx)\n",
        "\n",
        "    @classmethod\n",
        "    def load_valid_idx(cls):\n",
        "        return load_pickle(cls.valid_data_path)\n",
        "\n",
        "    def _init_mean_std(self, data):\n",
        "        # calculate mean and standard deviation of the data, e.g. x_train, initialise mean and std instance variables\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "        self.mean, self.std = np.mean(data), np.std(data)\n",
        "        self.save_meanstd()\n",
        "        return data\n",
        "\n",
        "    def standardise(self, array, to_float=False):\n",
        "        \"\"\"Standardise the given array.\n",
        "\n",
        "        The output array will have mean zero and standard deviation 1.\n",
        "\n",
        "        :param array: an array to be standardised\n",
        "        :param to_float: boolean parameter of whether to convert the input array to float\n",
        "        :return: standardised version of the input array\n",
        "        \"\"\"\n",
        "        if to_float:\n",
        "            array = np.array(array, dtype=np.float32)\n",
        "        if self.mean is None or self.std is None:\n",
        "            raise ValueError('No mean/std is initialised')\n",
        "\n",
        "        array -= self.mean\n",
        "        array /= self.std\n",
        "        return array\n",
        "\n",
        "    @classmethod\n",
        "    def norm_mask(cls, mask_array):\n",
        "        \"\"\"Convert an array with values in {0, 255} into an array with values in {0, 1}\n",
        "\n",
        "        :param mask_array: Input mask array of the shape (samples, rows, cols, channels). Values in {0, 255}\n",
        "        :return: the same array with values in {0, 1} - everything is divided by 255\n",
        "        \"\"\"\n",
        "        mask_array = np.array(mask_array, dtype=np.float32)\n",
        "        mask_array /= 255.0\n",
        "        return mask_array\n",
        "\n",
        "    @classmethod\n",
        "    def shuffle_train(cls, data, mask):\n",
        "        \"\"\"Create a random permutation of samples\n",
        "\n",
        "        :param data: 4D data tensor of train images, shape (samples, rows, cols, channels)\n",
        "        :param mask: 4D data tensor of train masks shape (samples, rows, cols, channels)\n",
        "        :return: a tuple (data, mask) with samples both data and mask tensors permuted\n",
        "        \"\"\"\n",
        "        perm = np.random.permutation(len(data))\n",
        "        data = data[perm]\n",
        "        mask = mask[perm]\n",
        "        return data, mask\n",
        "\n",
        "    @classmethod\n",
        "    def split_train_and_valid_by_patient(cls, data, mask, validation_split, shuffle=False):\n",
        "        \"\"\"Create a split of training data into training and validation data by patient\n",
        "\n",
        "        :param data: 4D data tensor of train images, shape (samples, rows, cols, channels)\n",
        "        :param mask: 4D data tensor of train masks shape (samples, rows, cols, channels)\n",
        "        :param validation_split: validation split, e.g. validation_split=0.2 will put 20% of the patients into validation set\n",
        "        :param shuffle: boolean variable, whether to shuffle the patient ids before choosing ids for validation\n",
        "        :return: a tuple of tuples (x_train, y_train), (x_valid, y_valid), where \"y\" stands for masks\n",
        "        \"\"\"\n",
        "        print('Shuffle & split...')\n",
        "        patient_nums = load_patient_num()\n",
        "        patient_dict = count_enum(patient_nums)\n",
        "        pnum = len(patient_dict)\n",
        "        val_num = int(pnum * validation_split)\n",
        "        patients = patient_dict.keys()\n",
        "        if shuffle:\n",
        "            random.shuffle(patients)\n",
        "        val_p, train_p = patients[:val_num], patients[val_num:]\n",
        "        train_indexes = [i for i, c in enumerate(patient_nums) if c in set(train_p)]\n",
        "        val_indexes = [i for i, c in enumerate(patient_nums) if c in set(val_p)]\n",
        "        x_train, y_train = data[train_indexes], mask[train_indexes]\n",
        "        x_valid, y_valid = data[val_indexes], mask[val_indexes]\n",
        "        cls.save_valid_idx(val_indexes)\n",
        "        print('val patients:', len(x_valid), val_p)\n",
        "        print('train patients:', len(x_train), train_p)\n",
        "        return (x_train, y_train), (x_valid, y_valid)\n",
        "\n",
        "    @classmethod\n",
        "    def split_train_and_valid(cls, data, mask, validation_split, shuffle=False):\n",
        "        \"\"\"Create a split of training data into training and validation data\n",
        "\n",
        "        :param data: 4D data tensor of train images, shape (samples, rows, cols, channels)\n",
        "        :param mask: 4D data tensor of train masks shape (samples, rows, cols, channels)\n",
        "        :param validation_split: validation split, e.g. validation_split=0.2 will put 20% of the images into validation set\n",
        "        :param shuffle: boolean variable, whether to shuffle the images before choosing some of them for validation\n",
        "        :return: a tuple of tuples (x_train, y_train), (x_valid, y_valid), where \"y\" stands for masks\n",
        "        \"\"\"\n",
        "        print('Shuffle & split...')\n",
        "        if shuffle:\n",
        "            data, mask = cls.shuffle_train(data, mask)\n",
        "        split_at = int(len(data) * (1. - validation_split))\n",
        "        x_train, x_valid = data[0:split_at, :, :, :], data[split_at:, :, :, :]\n",
        "        y_train, y_valid = mask[0:split_at, :, :, :], mask[split_at:, :, :, :]\n",
        "        cls.save_valid_idx(range(len(data))[split_at:])\n",
        "        print('type(x_train): ', type(x_train), 'type(x_valid): ', type(x_valid))\n",
        "        print('type(y_train): ', type(y_train), 'type(y_valid): ', type(y_valid))\n",
        "        return (x_train, y_train), (x_valid, y_valid)\n",
        "\n",
        "    def test(self, model, two_outputs): # two_outputs = False if classic U_net\n",
        "        \"\"\"Load, prepare and predict from the test data\n",
        "\n",
        "        :param model: compiled Model, e.g u-net to use\n",
        "        :param batch_size: predict images in batches of size=batch_size\n",
        "        \"\"\"\n",
        "        print('Loading and pre-processing test data...')\n",
        "        imgs_test = load_test_data()\n",
        "        imgs_test = preprocess(imgs_test)\n",
        "        imgs_test = self.standardise(imgs_test, to_float=True)\n",
        "\n",
        "        print('Loading best saved weights...')\n",
        "        if pretrained_path is not None:\n",
        "          model.load_weights(pretrained_path)\n",
        "        else:\n",
        "           model.load_weights(self.best_weight_path)\n",
        "\n",
        "        print('Predicting masks on test data and saving...')\n",
        "        imgs_mask_test = model.predict(imgs_test, batch_size=256, verbose=1)\n",
        "\n",
        "        if two_outputs == True:\n",
        "          np.save(self.test_mask_res, imgs_mask_test[0])\n",
        "          np.save(self.test_mask_exist_res, imgs_mask_test[1])\n",
        "        else:\n",
        "          np.save(self.test_mask_res, imgs_mask_test)\n",
        "\n",
        "    def fit(self, x_train, y_train, x_valid, y_valid, pretrained_path, two_outputs, data_augmentation): # two_outputs = False if classic U_net\n",
        "        \"\"\"Fit the model to the training data.\n",
        "        \n",
        "        For each predictor, there are 2 responses. The second response is binary nerve presence, created within the function.\n",
        "\n",
        "        :param x_train: 4D tensor of training images\n",
        "        :param y_train: 4D tensor of training masks\n",
        "        :param x_valid: 4D tensor of validation images\n",
        "        :param y_valid: 4D tensor of validation masks\n",
        "        :param pretrained_path: directory with \n",
        "        :return: fitted model\n",
        "        \"\"\"\n",
        "        print('Creating and compiling and fitting model...')\n",
        "\n",
        "        # create and compile the model - the Learning rate scheduler choice is very important here\n",
        "        if two_outputs:\n",
        "         if data_augmentation:\n",
        "           optimizer = RMSprop(learning_rate=1e-2, rho=0.9) # with data augmentation\n",
        "         else:\n",
        "           optimizer = Adam(lr=0.0045) # without data augmentation\n",
        "\n",
        "        if self.model_func == Nest_Net : \n",
        "          model = self.model_func()\n",
        "        else : \n",
        "          model = self.model_func(optimizer)\n",
        "\n",
        "        # checkpoints\n",
        "        model_checkpoint = ModelCheckpoint(self.__iter_res_file, monitor='val_loss')\n",
        "        model_save_best = ModelCheckpoint(self.best_weight_path, monitor='val_loss', save_best_only=True)\n",
        "        early_s = EarlyStopping(monitor='val_loss', patience=5, verbose=1) \n",
        "\n",
        "        # load pretrained model from a given path, if there is a pretrained model\n",
        "        load_pretrained_model(model, pretrained_path)\n",
        "\n",
        "        # y_train_2 and y_valid_2 are training and validation response arrays of nerve presence - needed for 2nd output\n",
        "        # shape (samples_train, ) and (samples_valid, ) respectively\n",
        "        print(\"type(y_train):\", type(y_train))\n",
        "        if two_outputs:\n",
        "          y_train_2 = get_object_existence(y_train)\n",
        "          y_valid_2 = get_object_existence(y_valid)\n",
        "          model.fit(\n",
        "              x_train, [y_train, y_train_2],\n",
        "              validation_data=(x_valid, [y_valid, y_valid_2]),\n",
        "              batch_size=64, epochs=50, # bs = 128, epochs = 50\n",
        "              verbose=1, shuffle=True,\n",
        "              callbacks=[model_save_best, model_checkpoint, early_s]\n",
        "          )\n",
        "        else: # Classic U_net: One output\n",
        "          model.fit(\n",
        "              x_train, y_train,\n",
        "              validation_data=(x_valid, y_valid),\n",
        "              batch_size=32, epochs=50, # bs = 32, epochs = 70 for U_net // bs = 32, epochs = 20 for r2u-net\n",
        "              verbose=1, shuffle=True,\n",
        "              callbacks=[model_save_best, model_checkpoint, early_s]\n",
        "          )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_and_predict(self, pretrained_path=None, split_random=True, two_outputs=True, data_augmentation=False): # two_outputs = False if classic U_net\n",
        "        \"\"\"Prepare the training data, fit the model on the train data, predict from the test data and save the results.\n",
        "\n",
        "        :param pretrained_path: the path to pretrained model, in case there is one\n",
        "        :param split_random: boolean, whether to split randomly or by patient\n",
        "        \"\"\"\n",
        "        self._dir_init()\n",
        "        \n",
        "        \n",
        "        if data_augmentation:\n",
        "          print('Loading and preprocessing and standardising augmented train data...')\n",
        "          imgs_train, imgs_mask_train = load_augmented_train_data()\n",
        "        else:\n",
        "          print('Loading and preprocessing and standardising train data...')\n",
        "          imgs_train, imgs_mask_train = load_train_data()\n",
        "        \n",
        "        imgs_train = preprocess(imgs_train)\n",
        "        imgs_mask_train = preprocess(imgs_mask_train)\n",
        "        imgs_mask_train = self.norm_mask(imgs_mask_train)\n",
        "        \n",
        "        # splitting the training data randomly or by patient\n",
        "        split_func = split_random and self.split_train_and_valid or self.split_train_and_valid_by_patient\n",
        "        (x_train, y_train), (x_valid, y_valid) = split_func(imgs_train, imgs_mask_train,\n",
        "                                                            validation_split=self.validation_split)\n",
        "        \n",
        "        # Important: validation data should be standardised using train data's mean and variance, otherwise we use some \n",
        "        # of the information about validation set during training\n",
        "        self._init_mean_std(x_train)\n",
        "        x_train = self.standardise(x_train, True)\n",
        "        x_valid = self.standardise(x_valid, True)\n",
        "        \n",
        "        # fitting the model\n",
        "        model = self.fit(x_train, y_train, x_valid, y_valid, pretrained_path, two_outputs, data_augmentation)\n",
        "        \n",
        "        # predicting on test data and saving the result\n",
        "        self.test(model, two_outputs)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------\n",
        "# if __name__ == '__main__':\n",
        "    # parser = OptionParser()\n",
        "    # parser.add_option(\"-s\", \"--split_random\", action='store', type='int', dest='split_random', default=1)\n",
        "    # parser.add_option(\"-m\", \"--model_name\", action='store', type='str', dest='model_name', default='u_model')\n",
        "    # #\n",
        "    # options, _ = parser.parse_args()\n",
        "    split_random = 1 #options.split_random\n",
        "    model_name = 'model_name' #options.model_name\n",
        "    if model_name is None:\n",
        "        raise ValueError('model_name is not defined')\n",
        "\n",
        "\n",
        "    #\n",
        "    ########## Model choice ########### need to comment the one unused\n",
        "\n",
        "    model_func = get_unet_classic\n",
        "    img_rows, img_cols = 96, 96\n",
        "    two_outputs = False\n",
        "    data_augmentation = False\n",
        "\n",
        "    # model_func = get_unet_inception_2head\n",
        "    # img_rows, img_cols = 80, 112\n",
        "    # two_outputs = True\n",
        "\n",
        "    # model_func = r2_unet\n",
        "    # img_rows, img_cols = 96, 96\n",
        "    # two_outputs = False\n",
        "\n",
        "        # Pre-trained model paths for models without data augmentation\n",
        "\n",
        "    pretrained_path = 'res/unet_2heads.hdf5' \n",
        "    # pretrained_path = 'res/r2unet.hdf5'\n",
        "    # pretrained_path = 'res/unet_classic.hdf5' \n",
        "\n",
        "    # Pre-trained model paths for models without data augmentation\n",
        "    # pretrained_path = 'res/unet_2heads_dataaug_rmsprop.hdf5'\n",
        "\n",
        "    # pretrained_path = None\n",
        "\n",
        "    #\n",
        "    lr = Learner(model_func, validation_split=0.2)\n",
        "    lr.train_and_predict(pretained_path = pretrained_path, split_random=split_random, two_outputs=two_outputs, data_augmentation=data_augmentation)\n",
        "    print('Results in ', lr.res_dir)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-448161a23ab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \"\"\"Perform training on the train data and predicting on the test data\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-448161a23ab3>\u001b[0m in \u001b[0;36mLearner\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# class variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mres_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'res'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mbest_weight_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nestnet_aug.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mtest_mask_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'imgs_mask_test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_dir' is not defined",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 2.2.0-rc1. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrUgmFb3-86n",
        "colab_type": "code",
        "outputId": "98291001-3a7a-4b45-8f34-4ba3be0458a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Change the model in model_func \n",
        "\n",
        "\n",
        "split_random = 1 #options.split_random\n",
        "model_name = 'model_name' #options.model_name\n",
        "if model_name is None:\n",
        "  raise ValueError('model_name is not defined')\n",
        "\n",
        "\n",
        "########## Model choice ########### need to comment the one unused\n",
        "\n",
        "model_func = Nest_Net\n",
        "# model_func = Attention_ResUNet\n",
        "img_rows, img_cols = 96, 96\n",
        "two_outputs = False\n",
        "# four_outputs = False\n",
        "data_augmentation = True\n",
        "\n",
        "    # model_func = get_unet_inception_2he ad\n",
        "    # img_rows, img_cols = 80, 112\n",
        "    # two_outputs = True\n",
        "\n",
        "    # model_func = r2_unet\n",
        "    # img_rows, img_cols = 96, 96\n",
        "    # two_outputs = False\n",
        "\n",
        "pretrained_path = 'res/nest_net.hdf5' \n",
        "# pretrained_path = 'res/attention.hdf5' \n",
        "\n",
        "\n",
        "    #\n",
        "lr = Learner(model_func, validation_split=0.2)\n",
        "lr.train_and_predict(pretrained_path = pretrained_path, split_random=split_random, two_outputs=two_outputs, data_augmentation=data_augmentation)\n",
        "print('Results in ', lr.res_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading and preprocessing and standardising augmented train data...\n",
            "Loading train data from /content/drive/My Drive/DLMI/data/np_data/augmented_imgs_train.npy and /content/drive/My Drive/DLMI/data/np_data/augmented_imgs_mask_train.npy\n",
            "(11270, 420, 580, 1)\n",
            "(11270, 420, 580, 1)\n",
            "Shuffle & split...\n",
            "type(x_train):  <class 'numpy.ndarray'> type(x_valid):  <class 'numpy.ndarray'>\n",
            "type(y_train):  <class 'numpy.ndarray'> type(y_valid):  <class 'numpy.ndarray'>\n",
            "Creating and compiling and fitting model...\n",
            "inputs: (None, 96, 96, 1)\n",
            "conv1_1 (None, 96, 96, 32)\n",
            "pool1 (None, 48, 48, 32)\n",
            "conv2_1 (None, 48, 48, 64)\n",
            "pool2 (None, 24, 24, 64)\n",
            "up1_2 (None, 96, 96, 32)\n",
            "conv1_2 (None, 96, 96, 64)\n",
            "type(y_train): <class 'numpy.ndarray'>\n",
            "Epoch 1/50\n",
            "282/282 [==============================] - 48s 169ms/step - loss: 0.2971 - dice_coef: 0.2503 - recall_16: 0.3746 - mean_io_u_12: 0.5527 - val_loss: 0.4146 - val_dice_coef: 0.0750 - val_recall_16: 0.0640 - val_mean_io_u_12: 0.5141\n",
            "Epoch 2/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: 0.0655 - dice_coef: 0.3954 - recall_16: 0.4081 - mean_io_u_12: 0.6172 - val_loss: 0.3482 - val_dice_coef: 0.0863 - val_recall_16: 0.0753 - val_mean_io_u_12: 0.5172\n",
            "Epoch 3/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.0072 - dice_coef: 0.4195 - recall_16: 0.4225 - mean_io_u_12: 0.6289 - val_loss: 0.3060 - val_dice_coef: 0.0852 - val_recall_16: 0.0689 - val_mean_io_u_12: 0.5176\n",
            "Epoch 4/50\n",
            "282/282 [==============================] - 48s 172ms/step - loss: -0.0626 - dice_coef: 0.4360 - recall_16: 0.4311 - mean_io_u_12: 0.6340 - val_loss: 0.2607 - val_dice_coef: 0.0960 - val_recall_16: 0.0766 - val_mean_io_u_12: 0.5199\n",
            "Epoch 5/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.1063 - dice_coef: 0.4479 - recall_16: 0.4388 - mean_io_u_12: 0.6408 - val_loss: 0.2418 - val_dice_coef: 0.0861 - val_recall_16: 0.0616 - val_mean_io_u_12: 0.5176\n",
            "Epoch 6/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.1425 - dice_coef: 0.4583 - recall_16: 0.4467 - mean_io_u_12: 0.6451 - val_loss: 0.2149 - val_dice_coef: 0.0888 - val_recall_16: 0.0637 - val_mean_io_u_12: 0.5181\n",
            "Epoch 7/50\n",
            "282/282 [==============================] - 45s 161ms/step - loss: -0.1634 - dice_coef: 0.4566 - recall_16: 0.4399 - mean_io_u_12: 0.6440 - val_loss: 0.1908 - val_dice_coef: 0.0920 - val_recall_16: 0.0669 - val_mean_io_u_12: 0.5193\n",
            "Epoch 8/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.1970 - dice_coef: 0.4698 - recall_16: 0.4531 - mean_io_u_12: 0.6504 - val_loss: 0.1700 - val_dice_coef: 0.0931 - val_recall_16: 0.0714 - val_mean_io_u_12: 0.5196\n",
            "Epoch 9/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.2169 - dice_coef: 0.4724 - recall_16: 0.4512 - mean_io_u_12: 0.6510 - val_loss: 0.1609 - val_dice_coef: 0.0870 - val_recall_16: 0.0637 - val_mean_io_u_12: 0.5178\n",
            "Epoch 10/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.2300 - dice_coef: 0.4705 - recall_16: 0.4590 - mean_io_u_12: 0.6501 - val_loss: 0.1359 - val_dice_coef: 0.0983 - val_recall_16: 0.0832 - val_mean_io_u_12: 0.5201\n",
            "Epoch 11/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.2419 - dice_coef: 0.4708 - recall_16: 0.4574 - mean_io_u_12: 0.6510 - val_loss: 0.1241 - val_dice_coef: 0.0981 - val_recall_16: 0.0751 - val_mean_io_u_12: 0.5206\n",
            "Epoch 12/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.2645 - dice_coef: 0.4804 - recall_16: 0.4632 - mean_io_u_12: 0.6556 - val_loss: 0.1150 - val_dice_coef: 0.0948 - val_recall_16: 0.0714 - val_mean_io_u_12: 0.5198\n",
            "Epoch 13/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.2780 - dice_coef: 0.4825 - recall_16: 0.4621 - mean_io_u_12: 0.6557 - val_loss: 0.1122 - val_dice_coef: 0.0870 - val_recall_16: 0.0604 - val_mean_io_u_12: 0.5173\n",
            "Epoch 14/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.2906 - dice_coef: 0.4845 - recall_16: 0.4633 - mean_io_u_12: 0.6566 - val_loss: 0.1051 - val_dice_coef: 0.0845 - val_recall_16: 0.0568 - val_mean_io_u_12: 0.5165\n",
            "Epoch 15/50\n",
            "282/282 [==============================] - 48s 169ms/step - loss: -0.3018 - dice_coef: 0.4870 - recall_16: 0.4594 - mean_io_u_12: 0.6579 - val_loss: 0.1041 - val_dice_coef: 0.0771 - val_recall_16: 0.0507 - val_mean_io_u_12: 0.5152\n",
            "Epoch 16/50\n",
            "282/282 [==============================] - 45s 159ms/step - loss: -0.3009 - dice_coef: 0.4796 - recall_16: 0.4581 - mean_io_u_12: 0.6551 - val_loss: 0.0942 - val_dice_coef: 0.0808 - val_recall_16: 0.0541 - val_mean_io_u_12: 0.5155\n",
            "Epoch 17/50\n",
            "282/282 [==============================] - 45s 159ms/step - loss: -0.3115 - dice_coef: 0.4833 - recall_16: 0.4620 - mean_io_u_12: 0.6560 - val_loss: 0.0851 - val_dice_coef: 0.0834 - val_recall_16: 0.0574 - val_mean_io_u_12: 0.5168\n",
            "Epoch 18/50\n",
            "282/282 [==============================] - 45s 159ms/step - loss: -0.3244 - dice_coef: 0.4899 - recall_16: 0.4668 - mean_io_u_12: 0.6588 - val_loss: 0.0686 - val_dice_coef: 0.0932 - val_recall_16: 0.0672 - val_mean_io_u_12: 0.5196\n",
            "Epoch 19/50\n",
            "282/282 [==============================] - 45s 159ms/step - loss: -0.3272 - dice_coef: 0.4865 - recall_16: 0.4568 - mean_io_u_12: 0.6575 - val_loss: 0.0676 - val_dice_coef: 0.0896 - val_recall_16: 0.0619 - val_mean_io_u_12: 0.5180\n",
            "Epoch 20/50\n",
            "282/282 [==============================] - 45s 159ms/step - loss: -0.3412 - dice_coef: 0.4954 - recall_16: 0.4688 - mean_io_u_12: 0.6613 - val_loss: 0.0556 - val_dice_coef: 0.0958 - val_recall_16: 0.0716 - val_mean_io_u_12: 0.5202\n",
            "Epoch 21/50\n",
            "282/282 [==============================] - 45s 160ms/step - loss: -0.3425 - dice_coef: 0.4914 - recall_16: 0.4678 - mean_io_u_12: 0.6593 - val_loss: 0.0492 - val_dice_coef: 0.0977 - val_recall_16: 0.0740 - val_mean_io_u_12: 0.5199\n",
            "Epoch 22/50\n",
            "282/282 [==============================] - 44s 157ms/step - loss: -0.3456 - dice_coef: 0.4908 - recall_16: 0.4639 - mean_io_u_12: 0.6604 - val_loss: 0.0540 - val_dice_coef: 0.0897 - val_recall_16: 0.0636 - val_mean_io_u_12: 0.5179\n",
            "Epoch 23/50\n",
            "282/282 [==============================] - 44s 157ms/step - loss: -0.3535 - dice_coef: 0.4952 - recall_16: 0.4678 - mean_io_u_12: 0.6616 - val_loss: 0.0567 - val_dice_coef: 0.0836 - val_recall_16: 0.0582 - val_mean_io_u_12: 0.5170\n",
            "Epoch 24/50\n",
            "282/282 [==============================] - 44s 157ms/step - loss: -0.3601 - dice_coef: 0.4987 - recall_16: 0.4726 - mean_io_u_12: 0.6623 - val_loss: 0.0567 - val_dice_coef: 0.0804 - val_recall_16: 0.0555 - val_mean_io_u_12: 0.5160\n",
            "Epoch 25/50\n",
            "282/282 [==============================] - 44s 157ms/step - loss: -0.3571 - dice_coef: 0.4928 - recall_16: 0.4668 - mean_io_u_12: 0.6612 - val_loss: 0.0508 - val_dice_coef: 0.0841 - val_recall_16: 0.0586 - val_mean_io_u_12: 0.5168\n",
            "Epoch 26/50\n",
            "282/282 [==============================] - 44s 158ms/step - loss: -0.3591 - dice_coef: 0.4928 - recall_16: 0.4672 - mean_io_u_12: 0.6617 - val_loss: 0.0502 - val_dice_coef: 0.0813 - val_recall_16: 0.0579 - val_mean_io_u_12: 0.5165\n",
            "Epoch 00026: early stopping\n",
            "Loading and pre-processing test data...\n",
            "Loading test data from /content/drive/My Drive/DLMI/data/np_data/imgs_test.npy\n",
            "(5508, 420, 580, 1)\n",
            "Loading best saved weights...\n",
            "Predicting masks on test data and saving...\n",
            "22/22 [==============================] - 6s 266ms/step\n",
            "Results in  /content/drive/My Drive/DLMI/data/res\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0znzJDCaXclV",
        "colab_type": "text"
      },
      "source": [
        "# 5. Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oF9DB3JUvaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ====================================================================================================================\n",
        "# submission\n",
        "# ====================================================================================================================\n",
        "\n",
        "# standard-module imports\n",
        "from skimage.transform import resize\n",
        "from itertools import chain\n",
        "\n",
        "# # separate-module imports\n",
        "# from data import load_test_data\n",
        "\n",
        "\n",
        "def prep(img):\n",
        "    \"\"\"Prepare the image for to be used in a submission\n",
        "\n",
        "    :param img: 2D image\n",
        "    :return: resized version of an image\n",
        "    \"\"\"\n",
        "    img = img.astype('float32')\n",
        "    img = resize(img, (image_rows, image_cols), preserve_range=True)\n",
        "    img = (img > 0.5).astype(np.uint8)  # threshold\n",
        "    return img\n",
        "\n",
        "\n",
        "def run_length_enc(label):\n",
        "    \"\"\"Create a run-length-encoding of an image\n",
        "\n",
        "    :param label: image to be encoded\n",
        "    :return: string with run-length-encoding of an image\n",
        "    \"\"\"\n",
        "    x = label.transpose().flatten()\n",
        "    y = np.where(x > 0)[0]\n",
        "\n",
        "    # consider empty all masks with less than 10 pixels being greater than 0\n",
        "    if len(y) < 10:\n",
        "        return ''\n",
        "\n",
        "    z = np.where(np.diff(y) > 1)[0]\n",
        "    start = np.insert(y[z + 1], 0, y[0])\n",
        "    end = np.append(y[z], y[-1])\n",
        "    length = end - start\n",
        "    res = [[s + 1, l + 1] for s, l in zip(list(start), list(length))]\n",
        "    res = list(chain.from_iterable(res))\n",
        "    return ' '.join([str(r) for r in res])\n",
        "\n",
        "\n",
        "def submission(two_outputs=True): \n",
        "    \"\"\"Create a submission .csv file.\n",
        "\n",
        "    The file will have 2 cols: img, pixels.\n",
        "        The image column consists of the ids of test images.\n",
        "        The pixels column consists of the run-length-encodings of the corresponding images.\n",
        "    \"\"\"\n",
        "    imgs_id_test = load_test_ids()\n",
        "    \n",
        "    print('Loading test_mask_res from %s' % Learner.test_mask_res)\n",
        "    imgs_test = np.load(Learner.test_mask_res)\n",
        "    if two_outputs:\n",
        "      print('Loading imgs_exist_test from %s' % Learner.test_mask_exist_res) # for the modified U_net with 2 outpouts\n",
        "      imgs_exist_test = np.load(Learner.test_mask_exist_res) # for the modified U_net with 2 outpouts\n",
        "\n",
        "    argsort = np.argsort(imgs_id_test)\n",
        "    imgs_id_test = imgs_id_test[argsort]\n",
        "    print(len(imgs_test))\n",
        "    imgs_test = imgs_test[argsort]\n",
        "    if two_outputs:\n",
        "      imgs_exist_test = imgs_exist_test[argsort] # for the modified U_net with 2 outpouts\n",
        "\n",
        "    total = imgs_test.shape[0]\n",
        "    ids = []\n",
        "    rles = []  # run-length-encodings\n",
        "    for i in range(total):\n",
        "        img = imgs_test[i, :, :, 0]\n",
        "        img = prep(img)\n",
        "\n",
        "        if two_outputs:\n",
        "          # new probability of nerve presence\n",
        "          img_exist = imgs_exist_test[i] # for the modified U_net with 2 outpouts\n",
        "          new_prob = (img_exist + min(1, np.sum(img) / 10000.0) * 5 / 3) / 2 # for the modified U_net with 2 outpouts\n",
        "          # setting mask to array of zeros if new probability of nerve presence < 0.5\n",
        "          if np.sum(img) > 0 and new_prob < 0.5: # for the modified U_net with 2 outpouts\n",
        "              img = np.zeros((image_rows, image_cols))\n",
        "\n",
        "        # producing run-length encoded version of the image\n",
        "        rle = run_length_enc(img)\n",
        "\n",
        "        rles.append(rle)\n",
        "        ids.append(imgs_id_test[i])\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('{}/{}'.format(i, total))\n",
        "\n",
        "    # creating a submission file\n",
        "    file_name = os.path.join(_dir, 'submission_nestnet.csv')\n",
        "    with open(file_name, 'w+') as f:\n",
        "        f.write('img,pixels\\n')\n",
        "        for i in range(total):\n",
        "            s = str(ids[i]) + ',' + rles[i]\n",
        "            f.write(s + '\\n')\n",
        "\n",
        "\n",
        "--------------------------------------------------------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    submission(two_outputs=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCOchrOYbHRg",
        "colab_type": "text"
      },
      "source": [
        "# 6. Qualitative results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESTTmRYUZII6",
        "colab_type": "code",
        "outputId": "4859015f-f07b-4f0c-ea21-3fadec5d4141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def standardise(array, mean, std, to_float=False):\n",
        "    \"\"\"Standardise the given array.\n",
        "\n",
        "    The output array will have mean zero and standard deviation 1.\n",
        "\n",
        "    :param array: an array to be standardised\n",
        "    :param to_float: boolean parameter of whether to convert the input array to float\n",
        "    :return: standardised version of the input array\n",
        "    \"\"\"\n",
        "    if to_float:\n",
        "        array = np.array(array, dtype=np.float32)\n",
        "\n",
        "    array -= mean\n",
        "    array /= std\n",
        "    return array\n",
        "\n",
        "print(\"Load Data...\")\n",
        "trainImages, trainMasks = load_train_data()\n",
        "\n",
        "X_test, y_test = trainImages[0:100,:], trainMasks[0:100,:] # 100 first images with their masks\n",
        "\n",
        "print('Loading and pre-processing test data...')\n",
        "imgs_test = X_test\n",
        "imgs_test = preprocess(imgs_test)\n",
        "mean, std = load_pickle('res/meanstd.dump') # 99.88485 56.971313\n",
        "imgs_test = standardise(imgs_test, mean, std, to_float=True)\n",
        "\n",
        "\n",
        "model = Nest_Net(lr = 2e-4) # best model\n",
        "img_rows, img_cols = 96,96\n",
        "print('Loading best saved weights...')\n",
        "load_pretrained_model(model, pretrained_path='res/nestnet_aug.hdf5') \n",
        "\n",
        "print('Predicting masks on test data...')\n",
        "imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
        "\n",
        "idx = [7, 8, 15]\n",
        "\n",
        "plt.figure(300)\n",
        "\n",
        "# First prediction\n",
        "plt.subplot(331)\n",
        "plt.imshow(X_test[idx[0],:,:,0],cmap='gray') # image\n",
        "\n",
        "plt.subplot(332)\n",
        "plt.imshow(y_test[idx[0],:,:,0],cmap='gray') # ground truth mask\n",
        "plt.title('ground truth')\n",
        "\n",
        "plt.subplot(333)\n",
        "plt.imshow(prep(imgs_mask_test[idx[0], :, :,0]),cmap='gray') # predicted mask\n",
        "plt.title('prediction')\n",
        "\n",
        "\n",
        "# Second prediction\n",
        "plt.subplot(334)\n",
        "plt.imshow(X_test[idx[1],:,:,0],cmap='gray')\n",
        "\n",
        "plt.subplot(335)\n",
        "plt.imshow(y_test[idx[1],:,:,0],cmap='gray')\n",
        "\n",
        "plt.subplot(336)\n",
        "plt.imshow(prep(imgs_mask_test[idx[1], :, :, 0]),cmap='gray')\n",
        "\n",
        "\n",
        "# Third prediction\n",
        "plt.subplot(337)\n",
        "plt.imshow(X_test[idx[2],:,:,0],cmap='gray')\n",
        "plt.subplot(338)\n",
        "plt.imshow(y_test[idx[2],:,:,0],cmap='gray')\n",
        "\n",
        "plt.subplot(339)\n",
        "plt.imshow(prep(imgs_mask_test[idx[2], :, :, 0]),cmap='gray')\n",
        "\n",
        "\n",
        "for i, id in enumerate(idx):\n",
        "  dice = np_dice_coef(prep(y_test[id,:,:,0]), prep(imgs_mask_test[id, :, :, 0]), smooth=1)\n",
        "  print(\"The dice coefficient for the line {} is {}\".format(i, round(dice,2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load Data...\n",
            "Loading train data from /content/drive/My Drive/DLMI/data/np_data/imgs_train.npy and /content/drive/My Drive/DLMI/data/np_data/imgs_mask_train.npy\n",
            "Loading and pre-processing test data...\n",
            "(100, 420, 580, 1)\n",
            "inputs: (None, 96, 96, 1)\n",
            "conv1_1 (None, 96, 96, 32)\n",
            "pool1 (None, 48, 48, 32)\n",
            "conv2_1 (None, 48, 48, 64)\n",
            "pool2 (None, 24, 24, 64)\n",
            "up1_2 (None, 96, 96, 32)\n",
            "conv1_2 (None, 96, 96, 64)\n",
            "Loading best saved weights...\n",
            "Predicting masks on test data...\n",
            "4/4 [==============================] - 12s 3s/step\n",
            "The dice coefficient for the line 0 is 0.87\n",
            "The dice coefficient for the line 1 is 0.69\n",
            "The dice coefficient for the line 2 is 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAEICAYAAAB7+s71AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEhISYwJZsCUkJCTGBLJgS0hISIwJ/hf/11g3lfoK+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkfWPxfYUUz8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}